{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkDi7S2rL1Ax"
      },
      "outputs": [],
      "source": [
        "#STEP 1\n",
        "#Download pre-trained embeddings\n",
        "\n",
        "!gdown --id 0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "word_vector_file_path = \"/content/GoogleNews-vectors-negative300.bin.gz\"\n",
        "model = KeyedVectors.load_word2vec_format(word_vector_file_path, binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 1\n",
        "#Method 2\n",
        "# Run this if first cell shows error\n",
        "import gensim.downloader\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "model = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-GMuYM_LncG",
        "outputId": "8a6cdd19-38b6-4e8d-c085-eb01b68b1cc4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 2\n",
        "# Word Vectors using gensim\n",
        "\n",
        "# word 'cementary' is excluded as it is not present in the word2vec pretrained model\n",
        "n_words = ['asylum','autograph','automobile','boy','bird','brother','cord','cushion','coast','crane','car','cock','fruit','furnace','forest','food','grin','graveyard','glass','gem','hill','implement','jewel','journey','lad','monk','midday','madhouse','mound','magician','noon','oracle','pillow','rooster','smile','string','sage','shore','serf','slave','signature','stove','tumbler','tool','voyage','wizard','woodland']\n",
        "\n",
        "wordvec =[]\n",
        "\n",
        "for word in n_words:\n",
        "  wordvec.append(model[word])\n"
      ],
      "metadata": {
        "id": "jlgz9STzMxWL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 3\n",
        "#Cosine Distance\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "S_word_vec=[]\n",
        "word_pair1=['cord','rooster','noon','fruit','autograph','automobile','mound','grin','graveyard','glass','boy','cushion','monk','coast','grin','shore','monk','boy','automobile','mound','lad','forest','food','shore','bird','coast','furnace','crane','hill','car','glass','magician','crane','brother','sage','oracle','bird','bird','food','brother','furnace','magician','hill','cord','glass','grin','journey','autograph','coast','forest','implement','cock','boy','cushion','automobile','midday','gem','asylum','asylum','asylum','serf']\n",
        "word_pair2=['smile','voyage','string','furnace','shore','wizard','stove','implement','madhouse','magician','rooster','jewel','slave','forest','lad','woodland','oracle','sage','cushion','shore','wizard','graveyard','rooster','voyage','woodland','hill','implement','rooster','woodland','journey','jewel','oracle','implement','lad','wizard','sage','crane','cock','fruit','monk','stove','wizard','mound','string','tumbler','smile','voyage','signature','shore','woodland','tool','rooster','lad','pillow','car','noon','jewel','fruit','monk','madhouse','slave']\n",
        "S_Manual=[0.02,0.04,0.04,0.05,0.06,0.11,0.14,0.18,0.42,0.44,0.44,0.45,0.57,0.85,0.88,0.9,0.91,0.96,0.97,0.97,0.99,1,1.09,1.22,1.24,1.26,1.37,1.41,1.48,1.55,1.78,1.82,2.37,2.41,2.46,2.61,2.63,2.63,2.69,2.74,3.11,3.21,3.29,3.41,3.45,3.46,3.58,3.59,3.6,3.65,3.66,3.68,3.82,3.84,3.92,3.94,3.94,0.19,0.39,3.46,3.04]\n",
        "\n",
        "\n",
        "for i in range(len(word_pair1)):\n",
        "  w1 = word_pair1[i]\n",
        "  w2 = word_pair2[i]\n",
        "  index_w1 = n_words.index(w1)\n",
        "  index_w2 = n_words.index(w2)\n",
        "\n",
        "  S_word_vec.append(cosine_similarity(wordvec[index_w1].reshape(1,300),wordvec[index_w2].reshape(1,300))[0][0])"
      ],
      "metadata": {
        "id": "1h173ZwWS5HE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 3\n",
        "#Pearson Correlation\n",
        "from scipy import stats\n",
        "\n",
        "P_word_vec = stats.pearsonr(S_Manual, S_word_vec)\n",
        "\n",
        "print('The Pearson Correlations are as follows:')\n",
        "print('Word2Vec: ',P_word_vec)"
      ],
      "metadata": {
        "id": "tIXnmoZ8VD92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f93f7d0-b149-4c34-c850-265b6b845fb5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Pearson Correlations are as follows:\n",
            "Word2Vec:  PearsonRResult(statistic=0.762628302486367, pvalue=9.210274605623711e-13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 4\n",
        "# Word Analogies\n",
        "\n",
        "#Reading the analogies\n",
        "file1 = open('./Analogies/top_5028_semantic_analogies.txt', 'r')\n",
        "sem_analogies = file1.readlines()\n",
        "\n",
        "file2 = open('./Analogies/top_5028_syntactic_analogies.txt', 'r')\n",
        "syn_analogies = file2.readlines()\n",
        "\n",
        "sem_analogy_words = [w.split() for w in sem_analogies]\n",
        "syn_analogy_words = [w.split() for w in syn_analogies]"
      ],
      "metadata": {
        "id": "OfM-MKUDVVhs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 4\n",
        "# Word Analogies - Word2Vec Analysis\n",
        "\n",
        "sem_analogy_accuracy = 0\n",
        "syn_analogy_accuracy = 0\n",
        "\n",
        "for i in range(len(sem_analogy_words)):\n",
        "  w1=model[sem_analogy_words[i][0]]\n",
        "  w2=model[sem_analogy_words[i][1]]\n",
        "  w3=model[sem_analogy_words[i][2]]\n",
        "  p_w4 = w3-w1+w2\n",
        "  gt=model[sem_analogy_words[i][3]]\n",
        "  sem_analogy_accuracy+=cosine_similarity(p_w4.reshape(1,300),gt.reshape(1,300))[0][0]\n",
        "\n",
        "sem_analogy_accuracy = sem_analogy_accuracy/len(sem_analogy_words)\n",
        "\n",
        "for i in range(len(syn_analogy_words)):\n",
        "  w1=model[syn_analogy_words[i][0]]\n",
        "  w2=model[syn_analogy_words[i][1]]\n",
        "  w3=model[syn_analogy_words[i][2]]\n",
        "  p_w4 = w3-w1+w2\n",
        "  gt=model[syn_analogy_words[i][3]]\n",
        "  syn_analogy_accuracy+=cosine_similarity(p_w4.reshape(1,300),gt.reshape(1,300))[0][0]\n",
        "\n",
        "syn_analogy_accuracy = syn_analogy_accuracy/len(syn_analogy_words)\n",
        "\n",
        "print('The semantic analogy accuracy using word2vec method is :',sem_analogy_accuracy*100, \" %.\")\n",
        "print('The syntactic analogy accuracy using word2vec method is :',syn_analogy_accuracy*100, \" %.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt8eLD5Rn-vS",
        "outputId": "b906f594-dc5f-4c28-b73e-ee7371673a55"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The semantic analogy accuracy using word2vec method is : 71.79592225964613  %.\n",
            "The syntactic analogy accuracy using word2vec method is : 60.75930756541075  %.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 4\n",
        "# Word Analogies - LSA Preprocessing\n",
        "\n",
        "# Creating the word vectors using LSA\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "nltk.download(\"brown\")\n",
        "nltk.download(\"stopwords\")\n",
        "from string import punctuation\n",
        "from nltk.corpus import brown\n",
        "from nltk import bigrams\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Getting High frequency words\n",
        "freq_of_words = Counter({})\n",
        "freq_bi = Counter({})\n",
        "stop_words = set(stopwords.words('english')+list(punctuation)+[\"``\",\"''\",\"--\"])\n",
        "biagrams = []\n",
        "for c in brown.categories():\n",
        "  words_wo_numbers = [w for w in brown.words(categories=c) if not w.isnumeric()]\n",
        "  words_wo_stop_words = [w for w in words_wo_numbers if not w.lower() in stop_words]\n",
        "  freq_of_words += Counter(words_wo_stop_words)\n",
        "  freq_bi += Counter(FreqDist(bigrams(words_wo_stop_words)))\n",
        "\n",
        "freq_of_words = dict(freq_of_words)\n",
        "\n",
        "all_freq_words = sorted(freq_of_words,key=freq_of_words.get,reverse=True)[:]\n",
        "highest_freq_words = all_freq_words[:5000]\n",
        "n_words = ['asylum','autograph','automobile','boy','bird','brother','cord','cushion','coast','cementary','crane','car','cock','fruit','furnace','forest','food','grin','graveyard','glass','gem','hill','implement','jewel','journey','lad','monk','midday','madhouse','mound','magician','noon','oracle','pillow','rooster','smile','string','sage','shore','serf','slave','signature','stove','tumbler','tool','voyage','wizard','woodland']\n",
        "words_in_brown_corpus = [ w for w in n_words if w in freq_of_words ]\n",
        "\n",
        "words=highest_freq_words+[w for w in words_in_brown_corpus if w not in highest_freq_words]\n",
        "\n",
        "M1= np.zeros((len(words),len(words)))\n",
        "# Constructing biagrams and calculating frequency\n",
        "for i in range(len(words)):\n",
        "  for j in range(len(words)):\n",
        "    bigram=(words[i],words[j])\n",
        "    M1[i][j] = freq_bi[(bigram)]\n",
        "\n",
        "#PPMI\n",
        "M1_plus = np.zeros_like(M1)\n",
        "total_bigrams = np.sum(M1)\n",
        "total_unigram_freq =0\n",
        "for i,w1 in enumerate(words):\n",
        "  total_unigram_freq+= freq_of_words[w1]\n",
        "\n",
        "for i,w1 in enumerate(words):\n",
        "  for j,w2 in enumerate(words):\n",
        "    prob_wc = M1[i][j]/total_bigrams\n",
        "    prob_w = freq_of_words[w1]/total_unigram_freq\n",
        "    prob_c = freq_of_words[w2]/total_unigram_freq\n",
        "    M1_plus[i][j] = max(np.log(prob_wc/(prob_c * prob_w)),0)\n",
        "\n",
        "#PCA\n",
        "#Scaled data\n",
        "M1_plus_scaled = preprocessing.scale(M1_plus.T)\n",
        "\n",
        "pca300 = PCA(n_components=300, svd_solver='full')\n",
        "pca300.fit(M1_plus_scaled)\n",
        "M2_300 = pca300.transform(M1_plus_scaled)\n",
        "\n",
        "LSA={}\n",
        "\n",
        "for i,w in enumerate(words):\n",
        "  LSA[w] = M2_300[i:i+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtQhhBAZW_Ic",
        "outputId": "f788b4d0-f75a-4481-e47f-dfadbcfe9a3d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "<ipython-input-17-0f61530ee9c8>:58: RuntimeWarning: divide by zero encountered in log\n",
            "  M1_plus[i][j] = max(np.log(prob_wc/(prob_c * prob_w)),0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 4\n",
        "# Word Analogies - LSA Analysis\n",
        "\n",
        "sem_analogy_accurancy = 0\n",
        "syn_analogy_accurancy = 0\n",
        "\n",
        "for i in range(len(sem_analogy_words)):\n",
        "  w1=LSA[sem_analogy_words[i][0]]\n",
        "  w2=LSA[sem_analogy_words[i][1]]\n",
        "  w3=LSA[sem_analogy_words[i][2]]\n",
        "  p_w4 = w3-w1+w2\n",
        "  gt=LSA[sem_analogy_words[i][3]]\n",
        "  sem_analogy_accuracy+=cosine_similarity(p_w4,gt)[0][0]\n",
        "\n",
        "sem_analogy_accuracy = sem_analogy_accuracy/len(sem_analogy_words)\n",
        "\n",
        "for i in range(len(syn_analogy_words)):\n",
        "  w1=LSA[syn_analogy_words[i][0]]\n",
        "  w2=LSA[syn_analogy_words[i][1]]\n",
        "  w3=LSA[syn_analogy_words[i][2]]\n",
        "  p_w4 = w3-w1+w2\n",
        "  gt=LSA[syn_analogy_words[i][3]]\n",
        "  syn_analogy_accuracy+=cosine_similarity(p_w4,gt)[0][0]\n",
        "\n",
        "syn_analogy_accuracy = syn_analogy_accuracy/len(syn_analogy_words)\n",
        "\n",
        "print('The semantic analogy accuracy using LSA method is :',sem_analogy_accuracy*100, \" %.\")\n",
        "print('The syntactic analogy accuracy using LSA method is :',syn_analogy_accuracy*100, \" %.\")"
      ],
      "metadata": {
        "id": "RpJAEwtax_Ah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "670c5b9c-4eae-45cb-ee12-ac710f62ba6f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The semantic analogy accuracy using LSA method is : 11.37295300245644  %.\n",
            "The syntactic analogy accuracy using LSA method is : 7.431095733440996  %.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Miscellaneous - Word analogies on larger set for Word2Vec\n",
        "\n",
        "file1 = open('./Analogies/full_c_semantic_analogies.txt', 'r')\n",
        "sem_analogies = file1.readlines()\n",
        "\n",
        "file2 = open('./Analogies/full_c_syntactic_analogies.txt', 'r')\n",
        "syn_analogies = file2.readlines()\n",
        "\n",
        "sem_analogy_words = [w.split() for w in sem_analogies]\n",
        "syn_analogy_words = [w.split() for w in syn_analogies]\n",
        "\n",
        "sem_analogy_accuracy = 0\n",
        "syn_analogy_accuracy = 0\n",
        "\n",
        "for i in range(len(sem_analogy_words)):\n",
        "  w1=model[sem_analogy_words[i][0]]\n",
        "  w2=model[sem_analogy_words[i][1]]\n",
        "  w3=model[sem_analogy_words[i][2]]\n",
        "  p_w4 = w3-w1+w2\n",
        "  gt=model[sem_analogy_words[i][3]]\n",
        "  sem_analogy_accuracy+=cosine_similarity(p_w4.reshape(1,300),gt.reshape(1,300))[0][0]\n",
        "\n",
        "sem_analogy_accuracy = sem_analogy_accuracy/len(sem_analogy_words)\n",
        "\n",
        "for i in range(len(syn_analogy_words)):\n",
        "  w1=model[syn_analogy_words[i][0]]\n",
        "  w2=model[syn_analogy_words[i][1]]\n",
        "  w3=model[syn_analogy_words[i][2]]\n",
        "  p_w4 = w3-w1+w2\n",
        "  gt=model[syn_analogy_words[i][3]]\n",
        "  syn_analogy_accuracy+=cosine_similarity(p_w4.reshape(1,300),gt.reshape(1,300))[0][0]\n",
        "\n",
        "syn_analogy_accuracy = syn_analogy_accuracy/len(syn_analogy_words)\n",
        "\n",
        "print('The semantic analogy accuracy using word2vec method is :',sem_analogy_accuracy*100, \" %.\")\n",
        "print('The syntactic analogy accuracy using word2vec method is :',syn_analogy_accuracy*100, \" %.\")"
      ],
      "metadata": {
        "id": "NhcLjmFE5jbx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9cf89c5-a582-4255-fb7d-aeb91eddde34"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The semantic analogy accuracy using word2vec method is : 63.855976842600725  %.\n",
            "The syntactic analogy accuracy using word2vec method is : 62.19563980708954  %.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HugAbw7zc1Yp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}