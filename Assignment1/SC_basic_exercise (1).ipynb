{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtR4WXxevhy-",
        "outputId": "101432b9-ca4e-4814-d8bb-a604b26bd3b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "nltk.download(\"brown\")\n",
        "nltk.download(\"stopwords\")\n",
        "from string import punctuation\n",
        "from nltk.corpus import brown\n",
        "from nltk import bigrams\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting High frequency words\n",
        "freq_of_words = Counter({})\n",
        "stop_words = set(list(punctuation)+[\"``\",\"''\",\"--\"])\n",
        "for c in brown.categories():\n",
        "  words_wo_numbers = [w for w in brown.words(categories=c) if not w.isnumeric()]\n",
        "  words_wo_stop_words = [w for w in words_wo_numbers if not w.lower() in stop_words]\n",
        "  freq_of_words += Counter(words_wo_stop_words)\n",
        "\n",
        "freq_of_words = dict(freq_of_words)\n",
        "\n",
        "highest_freq_words = sorted(freq_of_words,key=freq_of_words.get,reverse=True)[:5000]\n",
        "print(\"Most Common Unigrams : \\n1.\",highest_freq_words[0], \"\\n2.\", highest_freq_words[1],\"\\n3.\", highest_freq_words[2],\"\\n4.\", highest_freq_words[3],\"\\n5.\", highest_freq_words[4])\n",
        "print(\"Least Common Unigrams : \\n1.\",highest_freq_words[-1], \"\\n2.\", highest_freq_words[-2],\"\\n3.\", highest_freq_words[-3],\"\\n4.\", highest_freq_words[-4],\"\\n5.\", highest_freq_words[-5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fkw87n4wHk3",
        "outputId": "f421f09c-e91b-438c-b1fe-19a2c0512aea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Common Unigrams : \n",
            "1. the \n",
            "2. of \n",
            "3. and \n",
            "4. to \n",
            "5. a\n",
            "Least Common Unigrams : \n",
            "1. physiological \n",
            "2. similarly \n",
            "3. trends \n",
            "4. disposal \n",
            "5. relating\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New n words to be added to list of highest_freq_words\n",
        "n_words = ['asylum','autograph','automobile','boy','bird','brother','cord','cushion','coast','cementary','crane','car','cock','fruit','furnace','forest','food','grin','graveyard','glass','gem','hill','implement','jewel','journey','lad','monk','midday','madhouse','mound','magician','noon','oracle','pillow','rooster','smile','string','sage','shore','serf','slave','signature','stove','tumbler','tool','voyage','wizard','woodland']\n",
        "words_in_brown_corpus = [ w for w in n_words if w in freq_of_words ]"
      ],
      "metadata": {
        "id": "nLfAEh-j5tbR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=highest_freq_words+[w for w in words_in_brown_corpus if w not in highest_freq_words]"
      ],
      "metadata": {
        "id": "3tgcMBd4w8Gy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHaLFQSZlFkG",
        "outputId": "188b282b-4b56-48a0-c1b4-6bbc3ffec83f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5028"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "M1 = np.zeros((len(words),len(words)))"
      ],
      "metadata": {
        "id": "bbDBvUGXzXlX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constructing biagrams and calculating frequency\n",
        "for sent in brown.sents():\n",
        "  for i in range(1, len(sent)):\n",
        "    if sent[i-1] in words and sent[i] in words:\n",
        "            r = words.index(sent[i-1])\n",
        "            c = words.index(sent[i])\n",
        "            M1[r][c] += 1"
      ],
      "metadata": {
        "id": "Q_irCn8qBxEd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PPMI\n",
        "M1_plus = np.zeros_like(M1)\n",
        "total_bigrams = np.sum(M1)\n",
        "total_unigram_freq =0\n",
        "for i,w1 in enumerate(words):\n",
        "  total_unigram_freq+= freq_of_words[w1]\n",
        "\n",
        "for i,w1 in enumerate(words):\n",
        "  for j,w2 in enumerate(words):\n",
        "    if M1[i][j]==0:\n",
        "      continue\n",
        "    prob_wc = M1[i][j]/total_bigrams\n",
        "    prob_w = freq_of_words[w1]/total_unigram_freq\n",
        "    prob_c = freq_of_words[w2]/total_unigram_freq\n",
        "    M1_plus[i][j] = max(np.log(prob_wc/(prob_c * prob_w)),0)"
      ],
      "metadata": {
        "id": "QEkjAATVK8eU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SVD\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "svd = TruncatedSVD(n_components=300)\n",
        "M2_300 = svd.fit_transform(M1_plus)"
      ],
      "metadata": {
        "id": "y5L5qD6WOOco"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svd = TruncatedSVD(n_components=100)\n",
        "M2_100 = svd.fit_transform(M1_plus)\n",
        "\n",
        "svd = TruncatedSVD(n_components=10)\n",
        "M2_10 = svd.fit_transform(M1_plus)"
      ],
      "metadata": {
        "id": "cQvgUCWmBcnA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_pair1=['cord','rooster','noon','fruit','autograph','automobile','mound','grin','graveyard','glass','boy','cushion','monk','coast','grin','shore','monk','boy','automobile','mound','lad','forest','food','shore','bird','coast','furnace','crane','hill','car','glass','magician','crane','brother','sage','oracle','bird','bird','food','brother','furnace','magician','hill','cord','glass','grin','journey','autograph','coast','forest','implement','cock','boy','cushion','automobile','midday','gem']\n",
        "word_pair2=['smile','voyage','string','furnace','shore','wizard','stove','implement','madhouse','magician','rooster','jewel','slave','forest','lad','woodland','oracle','sage','cushion','shore','wizard','graveyard','rooster','voyage','woodland','hill','implement','rooster','woodland','journey','jewel','oracle','implement','lad','wizard','sage','crane','cock','fruit','monk','stove','wizard','mound','string','tumbler','smile','voyage','signature','shore','woodland','tool','rooster','lad','pillow','car','noon','jewel']\n",
        "S_Manual=[0.02,0.04,0.04,0.05,0.06,0.11,0.14,0.18,0.42,0.44,0.44,0.45,0.57,0.85,0.88,0.9,0.91,0.96,0.97,0.97,0.99,1,1.09,1.22,1.24,1.26,1.37,1.41,1.48,1.55,1.78,1.82,2.37,2.41,2.46,2.61,2.63,2.63,2.69,2.74,3.11,3.21,3.29,3.41,3.45,3.46,3.58,3.59,3.6,3.65,3.66,3.68,3.82,3.84,3.92,3.94,3.94]"
      ],
      "metadata": {
        "id": "-i3YYsZ_8jmw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cosine Similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "S_M1=[]\n",
        "S_M1_plus=[]\n",
        "S_M2_10=[]\n",
        "S_M2_100=[]\n",
        "S_M2_300=[]\n",
        "for i in range(len(word_pair1)):\n",
        "  w1 = word_pair1[i]\n",
        "  w2 = word_pair2[i]\n",
        "  index_w1 = words.index(w1)\n",
        "  index_w2 = words.index(w2)\n",
        "\n",
        "  S_M1.append(cosine_similarity(M1[index_w1:index_w1+1,:],M1[index_w2:index_w2+1,:])[0][0])\n",
        "  S_M1_plus.append(cosine_similarity(M1_plus[index_w1:index_w1+1,:],M1_plus[index_w2:index_w2+1,:])[0][0])\n",
        "  S_M2_10.append(cosine_similarity(M2_10[index_w1:index_w1+1,:],M2_10[index_w2:index_w2+1,:])[0][0])\n",
        "  S_M2_100.append(cosine_similarity(M2_100[index_w1:index_w1+1,:],M2_100[index_w2:index_w2+1,:])[0][0])\n",
        "  S_M2_300.append(cosine_similarity(M2_300[index_w1:index_w1+1,:],M2_300[index_w2:index_w2+1,:])[0][0])\n"
      ],
      "metadata": {
        "id": "I8Er1XP0DjGm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pearson Correlation\n",
        "from scipy import stats\n",
        "\n",
        "P_M1 = stats.pearsonr(S_Manual, S_M1)\n",
        "P_M1_plus = stats.pearsonr(S_Manual, S_M1_plus)\n",
        "P_M2_10 = stats.pearsonr(S_Manual, S_M2_10)\n",
        "P_M2_100 = stats.pearsonr(S_Manual, S_M2_100)\n",
        "P_M2_300 = stats.pearsonr(S_Manual, S_M2_300)"
      ],
      "metadata": {
        "id": "TdhrS0ojG5sp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('The Pearson Correlations are as follows:')\n",
        "print('M1: ',P_M1)\n",
        "print('M1+: ',P_M1_plus)\n",
        "print('M2_10: ',P_M2_10)\n",
        "print('M2_100: ',P_M2_100)\n",
        "print('M2_300: ',P_M2_300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzxvAuFDJdQE",
        "outputId": "bd3dedef-c3f2-4b83-cfdf-1b1f9589a02b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Pearson Correlations are as follows:\n",
            "M1:  PearsonRResult(statistic=0.32090296717895456, pvalue=0.014939690973575512)\n",
            "M1+:  PearsonRResult(statistic=0.19625095127612513, pvalue=0.14343815814364233)\n",
            "M2_10:  PearsonRResult(statistic=0.0975553563090073, pvalue=0.4703350358960295)\n",
            "M2_100:  PearsonRResult(statistic=0.3253228841861491, pvalue=0.013537965281917606)\n",
            "M2_300:  PearsonRResult(statistic=0.2579289633266839, pvalue=0.052733881030759794)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HP46BN_jWidn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}