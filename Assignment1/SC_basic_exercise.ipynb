{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtR4WXxevhy-",
        "outputId": "e1c7e325-3707-4308-c8c6-8d0328727c28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "nltk.download(\"brown\")\n",
        "nltk.download(\"stopwords\")\n",
        "from string import punctuation\n",
        "from nltk.corpus import brown\n",
        "from nltk import bigrams\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting High frequency words\n",
        "freq_of_words = Counter({})\n",
        "freq_bi = Counter({})\n",
        "stop_words = set(stopwords.words('english')+list(punctuation)+[\"``\",\"''\",\"--\"])\n",
        "biagrams = []\n",
        "for c in brown.categories():\n",
        "  words_wo_numbers = [w for w in brown.words(categories=c) if not w.isnumeric()]\n",
        "  words_wo_stop_words = [w for w in words_wo_numbers if not w.lower() in stop_words]\n",
        "  freq_of_words += Counter(words_wo_stop_words)\n",
        "  freq_bi += Counter(FreqDist(bigrams(words_wo_stop_words)))\n",
        "\n",
        "freq_of_words = dict(freq_of_words)\n",
        "\n",
        "highest_freq_words = sorted(freq_of_words,key=freq_of_words.get,reverse=True)[:5000]\n",
        "print(\"Most Common Unigrams : \\n1.\",highest_freq_words[0], \"\\n2.\", highest_freq_words[1],\"\\n3.\", highest_freq_words[2],\"\\n4.\", highest_freq_words[3],\"\\n5.\", highest_freq_words[4])\n",
        "print(\"Least Common Unigrams : \\n1.\",highest_freq_words[-1], \"\\n2.\", highest_freq_words[-2],\"\\n3.\", highest_freq_words[-3],\"\\n4.\", highest_freq_words[-4],\"\\n5.\", highest_freq_words[-5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fkw87n4wHk3",
        "outputId": "90cf7155-333d-4cf4-c2b4-e706935cc466"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Common Unigrams : \n",
            "1. one \n",
            "2. would \n",
            "3. said \n",
            "4. could \n",
            "5. time\n",
            "Least Common Unigrams : \n",
            "1. Salem \n",
            "2. improvements \n",
            "3. strategic \n",
            "4. dated \n",
            "5. Paula\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New n words to be added to list of highest_freq_words\n",
        "n_words = ['asylum','autograph','automobile','boy','bird','brother','cord','cushion','coast','cementary','crane','car','cock','fruit','furnace','forest','food','grin','graveyard','glass','gem','hill','implement','jewel','journey','lad','monk','midday','madhouse','mound','magician','noon','oracle','pillow','rooster','smile','string','sage','shore','serf','slave','signature','stove','tumbler','tool','voyage','wizard','woodland']\n",
        "words_in_brown_corpus = [ w for w in n_words if w in freq_of_words ]"
      ],
      "metadata": {
        "id": "nLfAEh-j5tbR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=highest_freq_words+[w for w in words_in_brown_corpus if w not in highest_freq_words]"
      ],
      "metadata": {
        "id": "3tgcMBd4w8Gy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHaLFQSZlFkG",
        "outputId": "e2c4ab13-e740-4e38-bd73-1fe7e8b60345"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5028"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "M1= np.zeros((len(words),len(words)))"
      ],
      "metadata": {
        "id": "bbDBvUGXzXlX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constructing biagrams and calculating frequency\n",
        "for i in range(len(words)):\n",
        "  for j in range(len(words)):\n",
        "    bigram=(words[i],words[j])\n",
        "    M1[i][j] = freq_bi[(bigram)]"
      ],
      "metadata": {
        "id": "Q_irCn8qBxEd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PPMI\n",
        "M1_plus = np.zeros_like(M1)\n",
        "total_bigrams = np.sum(M1)\n",
        "total_unigram_freq =0\n",
        "for i,w1 in enumerate(words):\n",
        "  total_unigram_freq+= freq_of_words[w1]\n",
        "\n",
        "for i,w1 in enumerate(words):\n",
        "  for j,w2 in enumerate(words):\n",
        "    prob_wc = M1[i][j]/total_bigrams\n",
        "    prob_w = freq_of_words[w1]/total_unigram_freq\n",
        "    prob_c = freq_of_words[w2]/total_unigram_freq\n",
        "    M1_plus[i][j] = max(np.log(prob_wc/(prob_c * prob_w)),0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEkjAATVK8eU",
        "outputId": "f87ed5e0-be8e-44e1-8290-77563083849b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-8c389760f9cb>:13: RuntimeWarning: divide by zero encountered in log\n",
            "  M1_plus[i][j] = max(np.log(prob_wc/(prob_c * prob_w)),0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PCA\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Scaled data\n",
        "M1_plus_scaled = preprocessing.scale(M1_plus.T)\n",
        "\n",
        "pca10 = PCA(n_components=10, svd_solver='full')\n",
        "pca10.fit(M1_plus_scaled)\n",
        "M2_10 = pca10.transform(M1_plus_scaled)"
      ],
      "metadata": {
        "id": "y5L5qD6WOOco"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca100 = PCA(n_components=100, svd_solver='full')\n",
        "pca100.fit(M1_plus_scaled)\n",
        "M2_100 = pca100.transform(M1_plus_scaled)\n",
        "#pca100_data[:5,:]"
      ],
      "metadata": {
        "id": "VKeid7l2k8s5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca300 = PCA(n_components=300, svd_solver='full')\n",
        "pca300.fit(M1_plus_scaled)\n",
        "M2_300 = pca300.transform(M1_plus_scaled)"
      ],
      "metadata": {
        "id": "OmlJRUUeorSs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_pair1=['cord','rooster','noon','fruit','autograph','automobile','mound','grin','graveyard','glass','boy','cushion','monk','coast','grin','shore','monk','boy','automobile','mound','lad','forest','food','shore','bird','coast','furnace','crane','hill','car','glass','magician','crane','brother','sage','oracle','bird','bird','food','brother','furnace','magician','hill','cord','glass','grin','journey','autograph','coast','forest','implement','cock','boy','cushion','automobile','midday','gem']\n",
        "word_pair2=['smile','voyage','string','furnace','shore','wizard','stove','implement','madhouse','magician','rooster','jewel','slave','forest','lad','woodland','oracle','sage','cushion','shore','wizard','graveyard','rooster','voyage','woodland','hill','implement','rooster','woodland','journey','jewel','oracle','implement','lad','wizard','sage','crane','cock','fruit','monk','stove','wizard','mound','string','tumbler','smile','voyage','signature','shore','woodland','tool','rooster','lad','pillow','car','noon','jewel']\n",
        "S_Manual=[0.02,0.04,0.04,0.05,0.06,0.11,0.14,0.18,0.42,0.44,0.44,0.45,0.57,0.85,0.88,0.9,0.91,0.96,0.97,0.97,0.99,1,1.09,1.22,1.24,1.26,1.37,1.41,1.48,1.55,1.78,1.82,2.37,2.41,2.46,2.61,2.63,2.63,2.69,2.74,3.11,3.21,3.29,3.41,3.45,3.46,3.58,3.59,3.6,3.65,3.66,3.68,3.82,3.84,3.92,3.94,3.94]"
      ],
      "metadata": {
        "id": "-i3YYsZ_8jmw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cosine Similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "S_M1=[]\n",
        "S_M1_plus=[]\n",
        "S_M2_10=[]\n",
        "S_M2_100=[]\n",
        "S_M2_300=[]\n",
        "for i in range(len(word_pair1)):\n",
        "  w1 = word_pair1[i]\n",
        "  w2 = word_pair2[i]\n",
        "  index_w1 = words.index(w1)\n",
        "  index_w2 = words.index(w2)\n",
        "\n",
        "  S_M1.append(cosine_similarity(M1[index_w1:index_w1+1,:],M1[index_w2:index_w2+1,:])[0][0])\n",
        "  S_M1_plus.append(cosine_similarity(M1_plus[index_w1:index_w1+1,:],M1_plus[index_w2:index_w2+1,:])[0][0])\n",
        "  S_M2_10.append(cosine_similarity(M2_10[index_w1:index_w1+1,:],M2_10[index_w2:index_w2+1,:])[0][0])\n",
        "  S_M2_100.append(cosine_similarity(M2_100[index_w1:index_w1+1,:],M2_100[index_w2:index_w2+1,:])[0][0])\n",
        "  S_M2_300.append(cosine_similarity(M2_300[index_w1:index_w1+1,:],M2_300[index_w2:index_w2+1,:])[0][0])\n"
      ],
      "metadata": {
        "id": "I8Er1XP0DjGm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pearson Correlation\n",
        "from scipy import stats\n",
        "\n",
        "P_M1 = stats.pearsonr(S_Manual, S_M1)\n",
        "P_M1_plus = stats.pearsonr(S_Manual, S_M1_plus)\n",
        "P_M2_10 = stats.pearsonr(S_Manual, S_M2_10)\n",
        "P_M2_100 = stats.pearsonr(S_Manual, S_M2_100)\n",
        "P_M2_300 = stats.pearsonr(S_Manual, S_M2_300)"
      ],
      "metadata": {
        "id": "TdhrS0ojG5sp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('The Pearson Correlations are as follows:')\n",
        "print('M1: ',P_M1)\n",
        "print('M1+: ',P_M1_plus)\n",
        "print('M2_10: ',P_M2_10)\n",
        "print('M2_100: ',P_M2_100)\n",
        "print('M2_300: ',P_M2_300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzxvAuFDJdQE",
        "outputId": "90634330-4975-44e8-872a-1f48831412c0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Pearson Correlations are as follows:\n",
            "M1:  PearsonRResult(statistic=0.21468858527181334, pvalue=0.1087767892623234)\n",
            "M1+:  PearsonRResult(statistic=0.2491029397810784, pvalue=0.06167924069984078)\n",
            "M2_10:  PearsonRResult(statistic=0.07014560147174698, pvalue=0.6041134458446543)\n",
            "M2_100:  PearsonRResult(statistic=0.2197109300074495, pvalue=0.10055564792606486)\n",
            "M2_300:  PearsonRResult(statistic=0.2549629828225948, pvalue=0.055614145541017346)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8HMAF8VPRmB0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}